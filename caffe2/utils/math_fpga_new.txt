#include "caffe2/utils/math.h"

#include <algorithm>
#include <array>
#include <atomic>
#include <chrono>
#include <cmath>
#include <cstdlib>
#include <cstring>
#include <functional>
#include <limits>
#include <numeric>
#include <random>
#include <tuple>
#include <unordered_set>
#include <vector>

#include "caffe2/core/context.h"
#include "caffe2/utils/cpu_neon.h"
#include "caffe2/utils/eigen_utils.h"
#include "caffe2/utils/fixed_divisor.h"

#include "Eigen/Core"
#include "Eigen/Dense"

//#ifdef CAFFE2_USE_MKL
#include <mkl.h>
//#endif // CAFFE2_USE_MKL

#ifdef CAFFE2_USE_HPTT
#include <hptt.h>
#endif // CAFFE2_USE_HPTT

#if defined(_MSC_VER)
#include <process.h>
#endif

//below are needed for FPGA kernel
#include <fstream>
#include <iostream>
#include "caffe2/core/context_fpga.h"
//from mm_utils CAFFE
//note bool is_a_ge_zero_and_a_lt_b(int a, int b); is implemetned
//in caffe2/utils/math/utils.h as
//MATH_UTILS_DECL bool IsAGeZeroAndALtB(const int a, const int b)
//#include <vector>
//#include "xcl2.hpp"
//#include "math.h"
//from mm_fpga CAFFE
//#include "caffe/caffe.hpp"
//#include "caffe/fpga/mm_utils.hpp"
//#include "xcl2.hpp"
//#include "math.h"
//important file for profiling
#include "caffe2/fpga/AsyncProfiler.hpp"


//Timer
//from https://github.com/pytorch/pytorch/blob/master/caffe2/core/timer.h
#include "caffe2/core/timer.h"

namespace caffe2 {

namespace math {

template <>
C10_EXPORT void Gemm<float, FPGAContext>(
    const CBLAS_TRANSPOSE trans_A,
    const CBLAS_TRANSPOSE trans_B,
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float* A,
    const float* B,
    const float beta,
    float* C,
    FPGAContext* context,
    TensorProto::DataType math_type) {
      //profiling
     //  #ifdef PROFILING
     //  std::ofstream profilingLog;
     //  //TODO: change a path
     //  profilingLog.open("~/home/log.csv", ios::app);
     //  profilingLog << M << "," << N << "," << K << ",";
     //
     //  //from line 276 and 277 of math_cpu.cc, format slightly different, now min is 1
     //  // int lda = (trans_A == CblasNoTrans) ? K : M;
     //  // int ldb = (trans_B == CblasNoTrans) ? N : K;
     // const int lda = std::max((trans_A == CblasNoTrans) ? K : M, 1);
     // const int ldb = std::max((trans_B == CblasNoTrans) ? N : K, 1);
     //
     // caffe2::Timer cpu;
     // double cpu_time = 0.0;
     // cpu.Start();
     //
     // cblas_sgemm(CblasRowMajor, trans_A, trans_B, M, N, K, alpha, A, lda, B, ldb, beta, C, N);
     // cpu_time += cpu.MicroSeconds();
     // profilingLog << cpu_time / 1000.0 << ",";
     // //record results from cblas_sgemm in cBlas
     // float cBlas[N*M];
     // for (int i=0; i<M; i++)
     // {
     //     for (int j=0; j<N; j++)
     //     {
     //         cBlas[i*N + j] = C[i*N+j];
     //     }
     // }
     // #endif

     double fpga_times[3];

     //CHECK: Is this better to be put before the kernel in mm_fpga.cc?
     // std::cout << "Setting up interfaces..." << std::endl;
     // std::vector<cl::Device> devices = xcl::get_xil_devices();
     // cl::Device device = devices[0];
     //
     // cl_int err;
     // OCL_CHECK(err, cl::Context context(device, NULL, NULL, NULL, &err));
     // OCL_CHECK(err, std::string deviceName = device.getInfo<CL_DEVICE_NAME>(&err));
     // //TODO:binary file
     // unsigned fileBufSize;
     // char* fileBuf = xcl::read_binary_file("matmul.xclbin", fileBufSize);
     // cl::Program::Binaries bins{{fileBuf,fileBufSize}};
     // devices.resize(1);
     // OCL_CHECK(err, cl::Program program(context, devices, bins, NULL, &err));
     // OCL_CHECK(err, cl::Kernel kernel(program, "matmul", &err));

     //passing by reference will not improve efficiency as A,B and C are arrays in form of ptrs
     //trans_A, trans_B, M,K,N, alpha and beta are constants
     Kernel(trans_A - 111, A, trans_B - 111, B, C, M, K, K, N, alpha, beta, fpga_times);

     // #ifdef PROFILING
     // #ifdef PROFILING_TIME
     // double total_time = 0.0;
     // for (unsigned i=0; i<3; ++i)
     // {
     //     total_time += fpga_times[i];
     // }
     // profilingLog << fpga_times[0] << "," << fpga_times[1] << "," << fpga_times[2] << "," << total_time << ",";
     // #endif
     // //mean square error
     // double mse = 0;
     // for (int i=0; i<M; i++)
     // {
     //     for (int j=0; j<N; j++)
     //     {
     //       mse += std::pow(std::fabs(cBlas[i*N+j] - C[i*N+j]) ,2);
     //     }
     // }
     // mse /= (N*M);
     //
     // profilingLog << mse << std::endl;
     // profilingLog.close();
     // #endif
}

// template <>
// C10_EXPORT void Gemm<float, CPUContext>(
//     const CBLAS_TRANSPOSE trans_A,
//     const CBLAS_TRANSPOSE trans_B,
//     const int M,
//     const int N,
//     const int K,
//     const float alpha,
//     const float* A,
//     const float* B,
//     const float beta,
//     float* C,
//     CPUContext* /*context*/,
//     TensorProto::DataType /*math_type*/) {
//   // MKL expects ld? >= 1
//   const int lda = std::max((trans_A == CblasNoTrans) ? K : M, 1);
//   const int ldb = std::max((trans_B == CblasNoTrans) ? N : K, 1);
//   cblas_sgemm(
//       CblasRowMajor,
//       trans_A,
//       trans_B,
//       M,
//       N,
//       K,
//       alpha,
//       A,
//       lda,
//       B,
//       ldb,
//       beta,
//       C,
//       N);
// }

template <>
C10_EXPORT void GemmEx<float, FPGAContext>(
    const CBLAS_TRANSPOSE trans_A,
    const CBLAS_TRANSPOSE trans_B,
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float* A,
    const int lda,
    const float* B,
    const int ldb,
    const float beta,
    float* C,
    const int ldc,
    FPGAContext* /*context*/) {
  cblas_sgemm(
      CblasRowMajor,
      trans_A,
      trans_B,
      M,
      N,
      K,
      alpha,
      A,
      lda,
      B,
      ldb,
      beta,
      C,
      ldc);
}

template <>
C10_EXPORT void Gemv<float, FPGAContext>(
    const CBLAS_TRANSPOSE trans_A,
    const int M,
    const int N,
    const float alpha,
    const float* A,
    const float* x,
    const float beta,
    float* y,
    FPGAContext* /*context*/,
    TensorProto::DataType /*math_type*/) {
    CAFFE_NOT_IMPLEMENTED;
  //cblas_sgemv(CblasRowMajor, trans_A, M, N, alpha, A, N, x, 1, beta, y, 1);
}

template <>
C10_EXPORT void GemmBatched<float, FPGAContext>(
    const CBLAS_TRANSPOSE trans_A,
    const CBLAS_TRANSPOSE trans_B,
    const int batch_size,
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float** A,
    const float** B,
    const float beta,
    float** C,
    FPGAContext* context,
    TensorProto::DataType /* math_type */) {
//#ifdef CAFFE2_USE_MKL
  //(void)context;
  // MKL expects ld? >= 1
  const int lda = std::max((trans_A == CblasNoTrans) ? K : M, 1);
  const int ldb = std::max((trans_B == CblasNoTrans) ? N : K, 1);
  const int ldc = std::max(N, 1);
  cblas_sgemm_batch(
      CblasRowMajor,
      &trans_A,
      &trans_B,
      &M,
      &N,
      &K,
      &alpha,
      A,
      &lda,
      B,
      &ldb,
      &beta,
      C,
      &ldc,
      1,
      &batch_size);
// #else // CAFFE2_USE_MKL
//   // loop over matrices in the batch
//   for (int i = 0; i < batch_size; ++i) {
//     math::Gemm<float, CPUContext>(
//         trans_A, trans_B, M, N, K, alpha, A[i], B[i], beta, C[i], context);
//   }
// #endif // CAFFE2_USE_MKL
}

template <>
C10_EXPORT void GemmStridedBatched<float, FPGAContext>(
    const CBLAS_TRANSPOSE trans_A,
    const CBLAS_TRANSPOSE trans_B,
    const int batch_size,
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float* A,
    const int A_stride,
    const float* B,
    const int B_stride,
    const float beta,
    float* C,
    const int C_stride,
    FPGAContext* context,
    TensorProto::DataType /* math_type */) {
//#ifdef CAFFE2_USE_MKL
  //(void)context;
  // MKL expects ld? >= 1
  const int lda = std::max((trans_A == CblasNoTrans) ? K : M, 1);
  const int ldb = std::max((trans_B == CblasNoTrans) ? N : K, 1);
  const int ldc = std::max(N, 1);
  std::vector<const float*> A_array(batch_size);
  std::vector<const float*> B_array(batch_size);
  std::vector<float*> C_array(batch_size);
  for (int i = 0; i < batch_size; ++i) {
    A_array[i] = A + i * A_stride;
    B_array[i] = B + i * B_stride;
    C_array[i] = C + i * C_stride;
  }
  cblas_sgemm_batch(
      CblasRowMajor,
      &trans_A,
      &trans_B,
      &M,
      &N,
      &K,
      &alpha,
      A_array.data(),
      &lda,
      B_array.data(),
      &ldb,
      &beta,
      C_array.data(),
      &ldc,
      1,
      &batch_size);
// #else // CAFFE2_USE_MKL
//   // loop over matrices in the batch
//   for (int i = 0; i < batch_size; ++i) {
//     math::Gemm<float, CPUContext>(
//         trans_A, trans_B, M, N, K, alpha, A, B, beta, C, context);
//     A += A_stride;
//     B += B_stride;
//     C += C_stride;
//   }
// #endif
}


template <>
C10_EXPORT void Im2ColNd<float, FPGAContext, StorageOrder::NCHW>(
    const int N,
    const int img_size,
    const int col_size,
    const int* img_shape,
    const int* col_shape,
    const int* kernel_shape,
    const int* stride,
    const int* dilation,
    const int* pad,
    const float* img_data,
    float* col_data,
    FPGAContext* /* context */,
    const int /* groups */) {
    std::cout<<"on fpga 1"<<std::endl;
}

template <>
C10_EXPORT void Col2ImNd<float, FPGAContext, StorageOrder::NCHW>(
    const int N,
    const int img_size,
    const int col_size,
    const int* img_shape,
    const int* col_shape,
    const int* kernel_shape,
    const int* stride,
    const int* dilation,
    const int* pad,
    const float* col_data,
    float* img_data,
    FPGAContext* /* context */,
    const int /* groups */) {
  // In NCHW, the number of groups doesn't affect Col2Im.
  //std::cout<<"on fpga 1-1"<<std::endl;
  CAFFE_NOT_IMPLEMENTED;
}

template <>
C10_EXPORT void Im2Col<float, FPGAContext, StorageOrder::NCHW>(
    const int C,
    const int H,
    const int W,
    const int kernel_h,
    const int kernel_w,
    const int dilation_h,
    const int dilation_w,
    const int pad_t,
    const int pad_l,
    const int pad_b,
    const int pad_r,
    const int stride_h,
    const int stride_w,
    const float* img_data,
    float* col_data,
    FPGAContext* context,
    const int /* groups */) {
      std::cout<<"on fpga 2"<<std::endl;
}

template <>
C10_EXPORT void Im2Col<float, FPGAContext, StorageOrder::NHWC>(
    const int C,
    const int H,
    const int W,
    const int kernel_h,
    const int kernel_w,
    const int dilation_h,
    const int dilation_w,
    const int pad_t,
    const int pad_l,
    const int pad_b,
    const int pad_r,
    const int stride_h,
    const int stride_w,
    const float* img_data,
    float* col_data,
    FPGAContext* context,
    const int groups) {
      std::cout<<"on fpga 3"<<std::endl;
}

/**
 * The layout of the result is N H W G R S C/G.
 * Note that groups are pulled out to an outer dimension so that we can use
 * GEMMs efficiently.
 */

template <>
C10_EXPORT void Im2ColNd<float, FPGAContext, StorageOrder::NHWC>(
    const int N,
    const int /*img_size*/,
    const int /*col_size*/,
    const int* img_shape,
    const int* col_shape,
    const int* kernel_shape,
    const int* stride,
    const int* dilation,
    const int* pad,
    const float* img_data,
    float* col_data,
    FPGAContext* /* context */,
    const int groups) {
    std::cout<<"fpga 4"<<std::endl;
}

template <>
C10_EXPORT void Col2Im<float, FPGAContext, StorageOrder::NCHW>(
    const int C,
    const int H,
    const int W,
    const int kernel_h,
    const int kernel_w,
    const int dilation_h,
    const int dilation_w,
    const int pad_t,
    const int pad_l,
    const int pad_b,
    const int pad_r,
    const int stride_h,
    const int stride_w,
    const float* col_data,
    float* img_data,
    FPGAContext* context,
    const int /* groups */) {std::cout<<"fpga 5"<<std::endl;}


template <>
C10_EXPORT void Col2Im<float, FPGAContext, StorageOrder::NHWC>(
    const int C,
    const int H,
    const int W,
    const int kernel_h,
    const int kernel_w,
    const int dilation_h,
    const int dilation_w,
    const int pad_t,
    const int pad_l,
    const int pad_b,
    const int pad_r,
    const int stride_h,
    const int stride_w,
    const float* col_data,
    float* img_data,
    CPUContext* context,
    const int groups) {
    std::cout<<"fpga 6"<<std::endl;
    return;
  }

template <>
C10_EXPORT void Col2ImNd<float, FPGAContext, StorageOrder::NHWC>(
    const int N,
    const int /*img_size*/,
    const int /*col_size*/,
    const int* img_shape,
    const int* col_shape,
    const int* kernel_shape,
    const int* stride,
    const int* dilation,
    const int* pad,
    const float* col_data,
    float* img_data,
    FPGAContext* context,
    const int groups) {CAFFE_NOT_IMPLEMENTED;}


} // namespace math
} // namespace caffe2
